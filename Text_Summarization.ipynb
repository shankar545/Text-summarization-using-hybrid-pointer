{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "import nltk\n",
        "from tensorflow.keras.layers import Concatenate, Dense, Dot, Embedding, LSTM, Bidirectional, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "import numpy as np\n",
        "from rouge import Rouge\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify the directory path\n",
        "directory_path = '/content/drive/My Drive/'\n",
        "\n",
        "# Specify the path to the .tgz file\n",
        "file_path = '/content/drive/My Drive/cnn_stories.tgz'\n",
        "\n",
        "# Extract the contents of the .tgz file\n",
        "with tarfile.open(file_path, 'r:gz') as tar:\n",
        "    tar.extractall('/content/data/')\n",
        "\n",
        "# List of file names\n",
        "file_names = [\n",
        "    '8278100e57ce63728df109f76b9888e7a918ad90.story',\n",
        "    '827811c8e01692a37b10247f3a3cd9ebc1ece71b.story',\n",
        "    # Add more file names here\n",
        "]\n",
        "\n",
        "# Split the data\n",
        "train_files, test_files = train_test_split(file_names, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the number of files in each set\n",
        "print(\"Number of training files:\", len(train_files))\n",
        "print(\"Number of testing files:\", len(test_files))\n",
        "\n",
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Preprocessing functions\n",
        "def preprocess_content(content):\n",
        "    # Tokenize the content using NLTK\n",
        "    tokens = nltk.word_tokenize(content)\n",
        "    return tokens\n",
        "\n",
        "# Preprocess the articles\n",
        "train_articles = []\n",
        "test_articles = []\n",
        "for file_name in train_files:\n",
        "    with open(os.path.join(directory_path, 'data', file_name), 'r') as file:\n",
        "        content = file.read()\n",
        "        train_articles.append(preprocess_content(content))\n",
        "\n",
        "for file_name in test_files:\n",
        "    with open(os.path.join(directory_path, 'data', file_name), 'r') as file:\n",
        "        content = file.read()\n",
        "        test_articles.append(preprocess_content(content))\n",
        "\n",
        "# Fit tokenizer on training data\n",
        "tokenizer.fit_on_texts(train_articles)\n",
        "\n",
        "# Convert the articles to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_articles)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_articles)\n",
        "\n",
        "# Pad the sequences to ensure equal length\n",
        "max_sequence_length = max(len(seq) for seq in train_sequences)\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Define the model architecture\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "hidden_units = 64\n",
        "\n",
        "# Encoder model\n",
        "encoder_inputs = tf.keras.Input(shape=(max_sequence_length,))\n",
        "encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = Bidirectional(LSTM(hidden_units, return_sequences=True))(encoder_embedding)\n",
        "\n",
        "# Attention mechanism\n",
        "attention = Dense(1, activation='tanh')(encoder_lstm)\n",
        "attention = tf.keras.layers.Flatten()(attention)\n",
        "attention = Activation('softmax')(attention)\n",
        "attention = tf.keras.layers.RepeatVector(hidden_units * 2)(attention)\n",
        "attention = tf.keras.layers.Permute([2, 1])(attention)\n",
        "\n",
        "# Apply attention weights\n",
        "sent_representation = tf.keras.layers.multiply([encoder_lstm, attention])\n",
        "sent_representation = tf.keras.layers.Lambda(lambda xin: tf.keras.backend.sum(xin, axis=-2))(sent_representation)\n",
        "\n",
        "# Decoder model\n",
        "decoder_inputs = tf.keras.Input(shape=(max_sequence_length,))\n",
        "decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(hidden_units, return_sequences=True)(decoder_embedding)\n",
        "decoder_output = Dense(vocab_size, activation='softmax')(decoder_lstm)\n",
        "\n",
        "# Compile the model\n",
        "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_output)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "model.fit([train_sequences, train_sequences], train_sequences, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss = model.evaluate([test_sequences, test_sequences], test_sequences)\n",
        "\n",
        "print(\"Test Loss:\", loss)\n",
        "\n",
        "# Generate summaries for test articles\n",
        "predicted_summaries = model.predict([test_sequences, test_sequences])\n",
        "\n",
        "# Convert predicted summaries back to text\n",
        "predicted_texts = []\n",
        "for summary in predicted_summaries:\n",
        "    predicted_text = tokenizer.sequences_to_texts([np.argmax(summary, axis=1)])[0]\n",
        "    predicted_texts.append(predicted_text)\n",
        "\n",
        "# Example reference and generated summaries\n",
        "reference_texts = [\"The estranged wife of former football star Deion Sanders was released from a Texas jail Friday, hours after she was booked on an assault charge, according to the Collin County Sheriff's Office.\"]\n",
        "generated_texts = [\"Deion Sanders' wife arrested on assault charge\"]\n",
        "\n",
        "rouge = Rouge()\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "scores = rouge.get_scores(generated_texts, reference_texts, avg=True)\n",
        "\n",
        "# Print ROUGE scores\n",
        "print(\"ROUGE scores:\")\n",
        "print(f\"ROUGE-1: {scores['rouge-1']}\")\n",
        "print(f\"ROUGE-2: {scores['rouge-2']}\")\n",
        "print(f\"ROUGE-L: {scores['rouge-l']}\")\n"
      ],
      "metadata": {
        "id": "tez_OxgPz5zy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}